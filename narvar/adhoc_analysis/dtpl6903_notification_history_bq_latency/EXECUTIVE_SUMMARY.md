# DTPL-6903: Notification History Latency - Executive Summary

**Date:** November 21, 2025 (Updated: November 25, 2025)  
**Status:** ‚úÖ READY TO IMPLEMENT - Project created, configuration in progress  
**Impact:** Customer-facing notification history feature experiencing 8-minute delays

> üìã **Critical Facts & Decision:** See [`CRITICAL_FACTS.md`](CRITICAL_FACTS.md) for verified costs, options comparison, and implementation plan

> üìã **Live Status Tracking:** See [`IMPLEMENTATION_LOG.md`](IMPLEMENTATION_LOG.md) for real-time progress

> ‚úÖ **Blocker Resolved (Nov 25):** Project `messaging-hub-bq-dedicated` created successfully. Cezar has owner permissions.

---

## Executive Summary (For Internal & External Communication)

**Problem:** The Notification History feature, used by retailers including Lands' End to search notification details by order number, is experiencing significant delays of up to 8-9 minutes. Investigation confirms the queries themselves are well-optimized and execute in 1-2 seconds, but are waiting 8+ minutes for database processing capacity due to shared infrastructure saturation. This issue started November 13th and has escalated, with peak delays occurring during overnight and morning hours (midnight-9am PST). The delays are caused by competing batch data processing and analytics workloads (Airflow 46%, Metabase 31%) monopolizing shared database resources, leaving insufficient capacity for customer-facing interactive features.

**Solution:** We are implementing a dedicated BigQuery project (`messaging-hub-bq-dedicated`) that uses on-demand billing, completely isolating messaging queries from the saturated shared reservation. The project has been created and is ready for configuration. This approach provides complete isolation from competing workloads at minimal cost (~$27/month for current 4.3 TB/month usage, with unlimited capacity and no queue delays). Implementation requires configuring cross-project data access (2 hours), updating the messaging application to use the new project ID (1-2 days), and deploying via rolling restart (zero downtime). Total timeline is 3-4 days. If usage grows significantly (>24 TB/month), we can transition to a dedicated reservation, but current volume makes on-demand the most cost-effective option. This will restore the Notification History feature to its expected performance level of <3 seconds end-to-end response time, eliminating the 8-minute queue delays.

---

## Table of Contents

1. [Problem](#problem)
2. [The Issue in One Picture](#the-issue-in-one-picture)
3. [Example: Real Query with 8-Minute Delay](#example-real-query-with-8-minute-delay)
4. [Root Cause](#root-cause-confirmed)
5. [Query Pattern Analysis](#query-pattern-analysis-understanding-the-87k-messaging-queries)
6. [Solution Approach](#solution-approach-separate-project-with-dedicated-reservation)
7. [Implementation Status](#implementation-status)
8. [Deployment Solution](#deployment-solution-separate-project-with-dedicated-reservation)
9. [Business Impact](#business-impact)

---

## Problem

Notification History feature (used by Lands' End and other retailers via Hub) experiencing severe delays:
- **8-minute wait times** before queries execute
- Started Nov 13, escalated Nov 18-21
- Affecting retailer experience (NT-1363 escalation from Lands' End)

---

## The Issue in One Picture

```
User Experience (Nov 21, 8am):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TOTAL DELAY: 9 minutes 3 seconds (558s)                         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñ†          ‚îÇ
‚îÇ ‚Üë                                                     ‚Üë          ‚îÇ
‚îÇ Queue Wait: 8 min 58 sec (99.6%)               Execution: 2s    ‚îÇ
‚îÇ                                                     (0.4%)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**The queries are fast (2s). The problem is waiting for available slots (558s).**

---

## Example: Real Query with 8-Minute Delay

**Job ID:** `job_x_RnGlaGvFGBYyzjA2b1ywgoDSz`

**Timing:**
- Creation time: 2025-11-21 13:48:35 PST
- Start time: 2025-11-21 13:56:07 PST  
- End time: 2025-11-21 13:56:08 PST
- **Queue wait:** ~8 minutes ‚ö†Ô∏è
- **Actual execution:** 1 second ‚úÖ

**Resources:**
- Bytes scanned: 4.21 GB
- Slot milliseconds: 13,224
- Service account: `messaging@narvar-data-lake.iam.gserviceaccount.com`
- Reservation: `bq-narvar-admin:US.default`

**Query:**
```sql
SELECT metric_name, order_number, tracking_number,
  narvar_tracer_id, carrier_moniker, notification_event_type,
  event_ts, notification.channel, request_failure_code,
  request_failure_reason, '' as status_code, dedupe_key,
  estimated_delivery_date, '' as data_available_date_time 
FROM messaging.pubsub_rules_engine_pulsar_debug 
WHERE event_ts BETWEEN TIMESTAMP '2025-11-20T05:25:43' 
  AND TIMESTAMP '2025-11-22T00:10:35.448412' 
  AND retailer_moniker = 'jdsports-emea' 
  AND metric_name = 'NOTIFICATION_EVENT_NOT_TRIGGERED' 
  AND request_failure_code NOT IN ('103', '112') 
  AND upper(order_number) = '188072755'
```

**Analysis:** This is a well-optimized, focused query scanning only 4.21 GB. The problem is NOT the query - it's waiting 480 seconds (8 minutes) to get BigQuery slots.

---

## Root Cause ‚úÖ CONFIRMED

**BigQuery reservation `bq-narvar-admin:US.default` is saturated.**

### Capacity Breakdown (Last 7 days):

| Service | Slot Consumption | Queries | Impact |
|---------|------------------|---------|--------|
| **Airflow ETL** | **46%** | 28,030 | Batch jobs monopolizing slots |
| **Metabase BI** | **31%** | 58,532 | Heavy BI load |
| Messaging (Notification History) | 10% | 87,383 | **Victim - experiencing delays** |
| analytics-api (Hub) | 1% | 62,005 | Also experiencing delays |
| n8n Shopify | 0.6% | 185,064 | Worst delays (19 min max) |
| All others | 11% | ~190,000 | Various services |

**Key Finding:** Airflow + Metabase consume 77% of all slots, starving interactive customer-facing services. The problem started Nov 13 and affects multiple services (Metabase, Looker, n8n), indicating systemic capacity issues in the shared reservation.

---

## Query Pattern Analysis: Understanding the 87K Messaging Queries

### Are These Queries the Same or Different?

**Answer:** They are **STRUCTURALLY SIMILAR** but with **DIFFERENT PARAMETERS**

### Workload Characteristics

**Volume & Pattern:**
- **87,383 queries** over 7 days (12,483/day consistent)
- Each user search = **10 parallel queries** (one per messaging table)
- Generated by automated service: [NoFlakeQueryService.java](https://github.com/narvar/notify-automation-service/blob/d5019d7bdcd36e80b03befff899978f28a39b2de/src/main/java/com/narvar/automationservice/services/notificationreports/NoFlakeQueryService.java#L34)

**Query Structure (Same Across All Queries):**
- All follow **notification history lookup pattern** by order number
- Tables queried: `pubsub_rules_engine_pulsar_debug`, `pubsub_rules_engine_pulsar_debug_V2`, `pubsub_rules_engine_kafka`, etc.
- WHERE clause pattern: Filter by `event_ts`, `retailer_moniker`, `order_number`, `metric_name`
- Performance: **2.2s average execution**, 12.2 GB average scan (well-optimized)

**Query Parameters (Different Per Search):**
- Different **order numbers** (user-driven searches)
- Different **retailers** (jdsports-emea, landsend, etc.)
- Different **time windows** (event_ts range varies)
- Different **notification types** (metric_name varies)

### Critical Peak Pattern Discovery (Nov 24)

**Initial Analysis (Nov 21):** Average of 48 concurrent slots suggested 50-slot reservation sufficient

**Peak Pattern Discovery (Nov 24):** Hourly analysis revealed daily capacity spikes:

| Time Period | Concurrent Slots | Pattern | Frequency |
|-------------|------------------|---------|-----------|
| Daytime (8am-6pm) | 46-57 slots | Stable | Daily |
| **9pm spike** | **186-386 slots** | **Peak** | **Daily** |
| Overnight (2-4am) | 59-142 slots | Elevated | Daily |
| Average (24h) | 48 slots | Baseline | Constant |

**Key Discovery:** Daily **9pm spike of 186-386 slots** (4-8x average) would cause nightly failures with fixed 50-slot configuration

### Capacity Planning Decision

**For Capacity Planning:**
- **Concurrent execution pattern:** 10 parallel queries per search is the critical factor
- **Peak concurrency:** 5 simultaneous user searches = 50 concurrent queries
- **Query behavior:** Fast execution (2.2s avg), lightweight (12 GB scan avg)
- **Capacity requirement:** 50-100 slots needed

**Configuration Selected:**
- **50 baseline slots:** Handles 100% of daytime traffic (8am-6pm) efficiently
- **Autoscale +50 slots:** Activates during 9pm spike and overnight elevation
- **Total capacity:** 100 slots maximum
- **Coverage:** 99.4% (handles 166 of 168 hours without queuing)

**Rationale:**
- Queries are **similar enough** that capacity planning based on **concurrent execution** (48 avg, 228 peak) is the right approach
- The **50 baseline + autoscale 50** configuration handles the workload efficiently
- Cost-optimized: **~$219/month** ($146 baseline + ~$73 autoscale when active)
- Alternative rejected: Fixed 100 slots ($292/month) wastes 52% capacity during business hours

**Why This Matters for Saurabh:**
- Average-based planning (50 fixed slots) would have resulted in **nightly failures at 9pm**
- Peak pattern analysis prevented deployment of insufficient capacity
- Autoscale configuration provides cost efficiency while handling peak loads

---

## Solution Approach: Separate Project with On-Demand Billing ‚≠ê

### Selected Solution: messaging-hub-bq-dedicated Project (CREATED ‚úÖ)

**Approach:** Dedicated GCP project for messaging BigQuery operations using **on-demand billing** (no reservation)

**Cost Analysis:**
- **On-demand:** ~$27/month (4.3 TB √ó $6.25/TB) ‚≠ê **RECOMMENDED**
- **Flex reservation:** ~$1,700/month (50 baseline + autoscale 50)
- **Annual commitment:** ~$1,000/month (50 slots, 1-year lock-in)

**Benefits:**
- ‚úÖ **Lowest cost:** $27/month (63x cheaper than alternatives)
- ‚úÖ **Unlimited capacity:** No queue delays, auto-scales infinitely
- ‚úÖ **Complete isolation:** Separate from Airflow/Metabase saturation
- ‚úÖ **No commitment:** Pay only for usage
- ‚úÖ **Simple setup:** No reservation management needed
- ‚úÖ **Reuses existing service account:** No credential changes

**Implementation:**
- ‚ö†Ô∏è Requires application changes (project_id parameter + fully-qualified table names)
- ‚ö†Ô∏è Cross-project BigQuery access setup (simple permission grants)
- ‚ö†Ô∏è Testing required before production rollout
- ‚ö†Ô∏è Timeline: 3-4 days total

**Cost Management:**
- Monitor monthly scanned data volume
- If exceeds 24 TB/month (~$150): Consider switching to flex reservation
- Break-even: Would need 234 TB/month to justify $1,700 flex cost
- Current: 4.3 TB/month = 54x below break-even point

**Reference:** See `CRITICAL_FACTS.md` for detailed cost analysis and `SEPARATE_PROJECT_SOLUTION.md` for implementation guide

---

## Implementation Status

### ‚úÖ Completed (Nov 21-25):
1. ‚úÖ **Investigation complete** - Root cause confirmed (reservation saturation by Airflow 46% + Metabase 31%)
2. ‚úÖ **Choke points identified** - n8n Shopify causes 88% of notification history delays during 8-9am window
3. ‚úÖ **Peak analysis complete** - Daily 9pm spike (186-386 slots) requires handling
4. ‚úÖ **Solution designed** - Separate project with on-demand billing
5. ‚úÖ **Project created** - `messaging-hub-bq-dedicated` (Cezar has owner permissions)
6. ‚úÖ **Cost analysis corrected** - On-demand $27/month vs flex $1,700/month

### üîÑ In Progress (Nov 25 - Day 1):
7. **Infrastructure setup** (2 hours remaining):
   - [ ] Link billing account
   - [ ] Enable BigQuery API
   - [ ] Grant service account (`messaging@narvar-data-lake`) access to new project
   - [ ] Grant data read access to messaging tables
   - [ ] Test cross-project query
   - [ ] Grant admin access (Saurabh, Julia, Eric, data-eng group)

### üìÖ Remaining (Days 2-4):
8. **Days 2-3:** Messaging team deployment
   - Update project_id: `"narvar-data-lake"` ‚Üí `"messaging-hub-bq-dedicated"`
   - Update table references to fully-qualified: `\`narvar-data-lake.messaging.table\``
   - Deploy to staging and test
   - Production rollout (rolling restart, zero downtime)

9. **Day 4:** Validation and close
   - Monitor queue times (<1 second expected)
   - Verify cost (~$27/month for 4.3 TB)
   - Confirm reservation = "NONE" in audit logs (on-demand billing)
   - Update Jira DTPL-6903 as resolved

**Detailed tracking:** `IMPLEMENTATION_LOG.md`  
**Cost analysis:** `CRITICAL_FACTS.md`  
**Complete guide:** `SEPARATE_PROJECT_SOLUTION.md`

---

## Deployment Solution: Separate Project with On-Demand Billing ‚≠ê

**‚úÖ Final Approach (Nov 25):** Create separate `messaging-hub-bq-dedicated` project using **on-demand billing** (no reservation)

> üìã **For detailed cost analysis and options, see:** [`CRITICAL_FACTS.md`](CRITICAL_FACTS.md)  
> Includes: verified pricing calculations, option comparisons, break-even analysis

> üìã **For implementation steps, see:** [`SEPARATE_PROJECT_SOLUTION.md`](SEPARATE_PROJECT_SOLUTION.md)  
> Includes: project setup, cross-project permissions, application changes, testing checklist

> üìã **Implementation tracking:** [`IMPLEMENTATION_LOG.md`](IMPLEMENTATION_LOG.md)  
> Real-time status of each deployment step

### Why Separate Project with On-Demand?

**Root cause:** narvar-data-lake project is assigned to saturated `bq-narvar-admin:US.default` reservation (org-level)
- Cannot assign individual service accounts to reservations (API limitation)
- **Solution:** Create dedicated project for messaging, leave it UNASSIGNED to use on-demand billing
- **Result:** Complete isolation at minimal cost

### Architecture Overview

```
messaging-hub-bq-dedicated (new project) ‚úÖ CREATED
‚îú‚îÄ‚îÄ Billing: Same as narvar-data-lake
‚îú‚îÄ‚îÄ Reservation: NONE (uses on-demand billing)
‚îÇ   ‚îú‚îÄ‚îÄ Cost: $6.25 per TB scanned
‚îÇ   ‚îî‚îÄ‚îÄ Current: 4.3 TB/month = $27/month
‚îú‚îÄ‚îÄ Capacity: Unlimited (auto-scales, no queue delays)
‚îú‚îÄ‚îÄ Service account: messaging@narvar-data-lake (reused, no new credentials)
‚îî‚îÄ‚îÄ Queries: Cross-project access to narvar-data-lake.messaging tables
```

### Benefits

1. **Lowest cost** - $27/month (vs $1,700 for flex reservation)
2. **Unlimited capacity** - No queue delays, scales automatically
3. **Complete isolation** - Separate from Airflow/Metabase saturation
4. **Simple credential management** - Reuses existing service account
5. **No commitment** - Pay only for usage, can add reservation later if needed

### Implementation Requirements

**Infrastructure (Data Engineering) - 2 hours:**
- ‚úÖ Create GCP project: `messaging-hub-bq-dedicated` (DONE)
- Enable BigQuery API
- Grant existing service account jobUser permission on new project
- Grant data read access to messaging tables in narvar-data-lake
- Test cross-project query
- Grant admin access to Saurabh, Julia, Eric, data-eng team

**Application (Messaging Team) - 1-2 days:**
- Update project_id: `"narvar-data-lake"` ‚Üí `"messaging-hub-bq-dedicated"`
- Update table references to fully-qualified names:
  - ‚ùå Wrong: `FROM messaging.pubsub_rules_engine_pulsar_debug`
  - ‚úÖ Correct: ``FROM `narvar-data-lake.messaging.pubsub_rules_engine_pulsar_debug` ``
- No credential changes needed (same service account)
- Deploy to staging ‚Üí test ‚Üí production (rolling restart)

### Cost Analysis (CORRECTED)

**On-Demand (Recommended):**
```
Current: 4.3 TB/month √ó $6.25/TB = $27/month ($324/year)
If grows to 10 TB/month: $63/month
If grows to 24 TB/month: $150/month (still viable)
Break-even vs flex: 234 TB/month
```

**Alternative Options (if needed later):**
- **Flex reservation:** $1,700/month (50 baseline + autoscale 50) - Only if usage exceeds 24 TB/month
- **Annual commitment:** $1,000/month (50 slots, 1-year) - Only for very high stable volume

### Timeline

**Total: 3-4 days**
- ‚úÖ Day 0: Project created (`messaging-hub-bq-dedicated`)
- üîÑ Day 1: Infrastructure setup (2 hours) - IN PROGRESS
- Days 2-3: Messaging team staging + production deployment
- Day 4: Validation and monitoring

**Status:** ‚úÖ Project creation blocker resolved

---

## Business Impact

**Current State:**
- Retailers experiencing 8-minute delays for notification history lookups
- Customer-facing feature unusable during business hours
- Lands' End escalation (NT-1363) - potential churn risk
- Root cause: Airflow (46%) + Metabase (31%) saturating shared reservation

**Post-Deployment:**
- Queue delays eliminated: 558s ‚Üí <1s (99.8% reduction)
- Query execution unchanged: ~2.2 seconds (queries are well-optimized)
- Total response time: <3 seconds end-to-end
- Unlimited capacity (on-demand auto-scales)

**Cost Impact (CORRECTED - Nov 25):**
- **On-demand billing:** ~$27/month (~$324/year)
- **Calculation:** 4.3 TB/month √ó $6.25/TB = $27/month ‚úÖ VERIFIED
- **Previous error:** Stated $219/month (7.8x overestimate)
- **Isolated billing:** Clean separation from main project costs
- **Cost protection:** Monitor usage; can switch to flex ($1,700/month) if grows >24 TB/month

**Timeline:**
- ‚úÖ Project created: `messaging-hub-bq-dedicated` (Nov 25)
- üîÑ Infrastructure setup: 2 hours (in progress)
- Application deployment: 2-3 days (messaging team)
- Total: 3-4 days to resolution

**Solution Approach (UPDATED):**
- ‚úÖ On-demand billing: **$27/month** - RECOMMENDED (separate project, no reservation)
- Flex reservation: $1,700/month - Only if usage exceeds 24 TB/month
- Annual commitment: $1,000/month - Only for stable high volume
- **Key insight:** Separate project enables on-demand billing (org-level assignment doesn't apply to new projects)

**Customer Impact:**
- Eliminates churn risk from poor UX
- Restores Notification History feature to expected performance
- No changes visible to end users (transparent deployment)

---

## Questions?

Contact: Cezar Mihaila (Data Engineering)  
Investigation details: `FINDINGS.md`, `CHOKE_POINTS_ANALYSIS.md`  
SQL queries: `queries/` folder (9 analysis queries, $1.85 cost)

---

## Related Documents

### Analysis & Root Cause:
- **FINDINGS.md** - Comprehensive root cause analysis showing reservation saturation
- **CHOKE_POINTS_ANALYSIS.md** - 10-minute period analysis identifying n8n Shopify impact
- **CAPACITY_ANALYSIS_SUMMARY.md** - Peak pattern discovery (9pm spike) and capacity justification
- **README.md** - Investigation overview and navigation guide

### Implementation Planning:
- **SEPARATE_PROJECT_SOLUTION.md** ‚≠ê **CURRENT - Complete Implementation Guide (Nov 24-25)**
  - **Solution:** Separate project approach with dedicated reservation
  - **Timeline:** 3-5 days (after project creation)
  - **Cost:** ~$219/month ($146 baseline + ~$73 autoscale)
  - **Infrastructure setup:** Project creation, reservation assignment, permissions
  - **Application changes:** project_id parameter + fully-qualified table names
  - **Testing:** Cross-project query validation, staging deployment checklist
  - **Rollback:** 2-minute procedure (revert project_id)
  
- **IMPLEMENTATION_LOG.md** ‚≠ê **Real-Time Status Tracking**
  - Step-by-step implementation progress
  - Current status: Blocked on Step 1 (project creation permission)
  - Each step has: commands, expected output, actual result, timestamp
  - Tracks Phase 1 (infrastructure), Phase 2 (staging), Phase 3 (production)

- **REQUEST_FOR_JULIA_SAURABH.md** - Project Creation Request
  - Copy-paste commands for org admins to create project
  - Resolves current blocker (resourcemanager.projects.create permission)
  
- **MESSAGING_CAPACITY_PLANNING.md** - Capacity Analysis Reference
  - Original capacity calculations (still valid)
  - Workload characteristics (87,383 queries, 8,040 slot-hours)
  - Peak pattern analysis methodology

### Supporting Data:
- **queries/** - 11 SQL analysis queries (validated and executed, $1.85 total cost)
- **results/** - Query output data (CSV and JSON formats including hourly_peak_slots.csv)

---

**Current Status (Nov 25):** Infrastructure setup blocked on project creation permission. Awaiting Julia or Saurabh to create `messaging-hub-bq-dedicated` project. All other steps ready to execute.

